{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ProgettoML.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kVd0tVl5iN-r",
        "L5lxwLgAuD1A",
        "JZgEMzUxtAwR",
        "6_kIPTSmvU-Z"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freguti/progettoML/blob/main/ProgettoML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXLginkggdlr"
      },
      "source": [
        "STEP:\n",
        "\n",
        "1 - Dovremo prendere, per ogni super categoria, tutte le annotazioni e associarle alla super categoria corrispondente.\n",
        "\n",
        "2 -  fare il fine-tuning del modello di CTRL tramite le annotazioni associate alla super categoria e generare del testo\n",
        "\n",
        "    2.1 - modficare i control code durante la fase di fine tuning, in modo tale da inserire i nostri.  \n",
        "\n",
        "3 - Valutare il testo generato tramite una versione modificata di TexyGen.\n",
        "\n",
        "Dubbi: \n",
        "- bisogna usare le categorie o le super categorie? \n",
        "\n",
        "Commenti:\n",
        "\n",
        "Al momento ho creato una demo che mostra le super categorie e prende le annotazioni di una categoria fissa.\n",
        "\n",
        "CTRL sono riuscito solo a farlo girare in modalità low memory, ma è molto più inefficiente\n",
        "\n",
        "\n",
        "**SIMONE:** guardare se la computazione della loss nella fase di fintuning sia corretta.\n",
        "\n",
        "provare ad eseguire il finetuning con il trainer\n",
        "\n",
        "la generazione genera parole a caso, non so se gli devo dare l'ultima parola generata o la frase generata fino a quel momento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6legMWDEsaUe"
      },
      "source": [
        "#Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTKOELnvWXzL"
      },
      "source": [
        "%%capture\n",
        "\n",
        "from google.colab import drive\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "import subprocess\n",
        "!pip install transformers\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import torch\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu,SmoothingFunction\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "NUM_LINES_PER_ITERATION = 15"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6h8s1YMzAa3"
      },
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXQ-b5feOEJW"
      },
      "source": [
        "#https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb demo coco API\n",
        "def init_COCO():           \n",
        "  dataDir='/content/drive/MyDrive/Machine Learning/Progetto'\n",
        "  dataType='val2017'\n",
        "  modelDir = dataDir + '/model/' \n",
        "  instFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
        "  annFile = '{}/annotations/captions_{}.json'.format(dataDir,dataType)\n",
        "  \n",
        "  coco=COCO(instFile) #modello con le foto e le categorie\n",
        "  coco_ann=COCO(annFile) #modello con le annotazioni\n",
        "  return coco,coco_ann\n",
        "\n",
        "def choose_cat(cats):\n",
        "  cat_choose = \"\"\n",
        "  while cat_choose not in cats and cat_choose != \"exit\":\n",
        "    cat_choose = input(\"Choose a category from this list {0}: \\nPrint 'exit' to quit\\n\" .format(cats))\n",
        "    if cat_choose not in cats and cat_choose != \"exit\":\n",
        "      print(\"This category is not valid\")\n",
        "  \n",
        "  print(\"category choosen: {0}\".format(cat_choose))\n",
        "  return cat_choose\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVd0tVl5iN-r"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDpxOgM4cuCx"
      },
      "source": [
        "# display COCO categories and supercategories\n",
        "def get_categories(coco):\n",
        "  print('Getting categories...')\n",
        "  cats = coco.loadCats(coco.getCatIds())\n",
        "  nms=[cat['name'] for cat in cats]\n",
        "\n",
        "  supernms = set([cat['supercategory'] for cat in cats])\n",
        "  return supernms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZFKe6TBc8Ao"
      },
      "source": [
        "# load and display caption annotations by category\n",
        "def get_annotation(coco,coco_ann,catNms = ['outdoor']):\n",
        "  print('Getting annotations for {0}'.format(catNms))\n",
        "  catIds = coco.getCatIds(catNms=catNms); #seleziono gli id delle categorie che voglio analizzare\n",
        "  imgIds = coco.getImgIds(catIds=catIds); #ottengo gli id delle immagini delle categorie desiderate tramite gli id delle categorie\n",
        "  annIds = coco_ann.getAnnIds(imgIds=imgIds); #ottengo gli id delle annotazioni relative alle immagini, tramite gli id delle immagini\n",
        "\n",
        "  anns = coco_ann.loadAnns(annIds)\n",
        "  return anns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojOzXbnHiYKP"
      },
      "source": [
        "# CTRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QEYXrT5vam3"
      },
      "source": [
        "## Huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViiFhH1Rv581"
      },
      "source": [
        "### Prepare model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1z3xlFJvCeJ"
      },
      "source": [
        "# --- PREPARE model, tokenizer and device\n",
        "\n",
        "# Labels da togliere\n",
        "class TextSet(torch.utils.data.Dataset): \n",
        "  def __init__(self, text, labels = \"\"):\n",
        "    print(\"TextSet inizializing...\")\n",
        "    self.labels = labels\n",
        "    self.text = text\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    label = self.labels[idx]\n",
        "    text = self.text[idx]\n",
        "    sample = {\"Text\": text, \"Class\": label}\n",
        "    return sample\n",
        "\n",
        "class TextSet_v3(torch.utils.data.Dataset): \n",
        "  def __init__(self, text):\n",
        "    self.text = text\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.text[idx]\n",
        "    return text\n",
        "\n",
        "def init(pretrained = False):\n",
        "  print(\"Start init CTRLLMHeadModel\")\n",
        "  from transformers import CTRLLMHeadModel, CTRLConfig, CTRLTokenizer\n",
        "  import torch\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  tokenizer = CTRLTokenizer.from_pretrained('ctrl')\n",
        "  if not pretrained:\n",
        "    model = CTRLLMHeadModel.from_pretrained('ctrl', config = CTRLConfig(n_layer = 9))\n",
        "    model.save_pretrained(\"/content/drive/MyDrive/Machine Learning/Progetto/Models/vanilla10\")\n",
        "  else:\n",
        "    #loaded_model = torch.jit.load(\"/content/drive/MyDrive/Machine Learning/Progetto/pretrained.pt\")\n",
        "    model = CTRLLMHeadModel.from_pretrained(\"/content/drive/MyDrive/Machine Learning/Progetto/Models/vanilla10\")\n",
        "  return model, tokenizer, device\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnh40DGfwAtw"
      },
      "source": [
        "### Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5796L2RwIku"
      },
      "source": [
        "def prepare_dataset_v2(annotations, category, tokenizer):\n",
        "  annotations_ = annotations[0:15000]\n",
        "  annotations_train = []\n",
        "  annotations_val = []\n",
        "  for idx in iter(range(len(annotations_))):\n",
        "    annotations_[idx] = category + \" \" + annotations_[idx]['caption']\n",
        "    #annotations_[idx] = tokenizer.encode(annotations_[idx], add_special_tokens=False, return_tensors=\"pt\")\n",
        "    if idx % 15 == 0:\n",
        "      annotations_val.append(tokenizer(annotations_[idx], padding=\"max_length\", add_special_tokens=False, return_tensors=\"pt\"))\n",
        "    else:\n",
        "      annotations_train.append(tokenizer(annotations_[idx], padding=\"max_length\" ,add_special_tokens=False, return_tensors=\"pt\"))\n",
        "  return annotations_train,annotations_val\n",
        "\n",
        "def finetune_v2(model,tokenizer,device,annotations,category = 'outdoor' ):#,dataset_path = \"/content/drive/MyDrive/Machine Learning/Progetto/prova_1_val.txt\"):\n",
        "  print(\"start finetune\")\n",
        "  train_dataset = []\n",
        "  model.to(device)\n",
        "  len_annotation = len(annotations)\n",
        "  print(len(annotations))\n",
        "  len_annotation = 15000\n",
        "  model.train()\n",
        "  #Se non esiste il control code lo aggiungo\n",
        "  encoded_category = tokenizer.encode(category, add_special_tokens=False)\n",
        "  if not any(encoded_category[0] == x for x in tokenizer.control_codes.values()):\n",
        "    tokenizer.control_codes[category] = encoded_category[0]\n",
        "  #separo i due dataset\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "  train_dataset,val_dataset = prepare_dataset_v2(annotations, category, tokenizer)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/Machine Learning/Progetto/results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=300,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='/content/drive/MyDrive/Machine Learning/Progetto/logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "  )\n",
        "  print(train_dataset)\n",
        "  trainer = Trainer(\n",
        "      model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "      args=training_args,                  # training arguments, defined above\n",
        "      train_dataset=train_dataset,         # training dataset\n",
        "      eval_dataset=val_dataset             # evaluation dataset\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "def format_dataset(annotations,start,category):\n",
        "  dataset = []\n",
        "  labels = []\n",
        "  end = start + NUM_LINES_PER_ITERATION\n",
        "  if len(annotations) < end:\n",
        "    end = len(annotations)\n",
        "  annotation = annotations[start:end ]\n",
        "  text = category\n",
        "  for ann in annotation:\n",
        "    text += \" \" + ann['caption']\n",
        "  dataset.append(text)\n",
        "  labels.append(category)\n",
        "  return dataset,end,labels\n",
        "\n",
        "def finetune(model,tokenizer,device,annotations,category = 'outdoor' ):#,dataset_path = \"/content/drive/MyDrive/Machine Learning/Progetto/prova_1_val.txt\"):\n",
        "  print(\"start finetune\")\n",
        "  train_dataset = []\n",
        "  model.to(device)\n",
        "  len_annotation = len(annotations)\n",
        "  model.train()\n",
        "  #Se non esiste il control code lo aggiungo\n",
        "  encoded_category = tokenizer.encode(category, add_special_tokens=False)\n",
        "  if not any(encoded_category[0] == x for x in tokenizer.control_codes.values()):\n",
        "    tokenizer.control_codes[category] = encoded_category[0]\n",
        "  for epoch in range(3):\n",
        "    print(\"EPOCH\")\n",
        "    index = 0\n",
        "    while index < len_annotation:\n",
        "      dataset,index,labels = format_dataset(annotations = annotations, start = index, category = category)\n",
        "      print(\"{0} / {1}\".format(index,len_annotation))\n",
        "      text_labels_df = pd.DataFrame({'Text': dataset, 'Labels': labels})\n",
        "      TD = TextSet(text_labels_df['Text'], text_labels_df['Labels'])\n",
        "                            \n",
        "      train_loader = DataLoader(TD, batch_size=1, shuffle=True)\n",
        "      optim = AdamW(model.parameters(), lr=5e-5)\n",
        "      for batch in train_loader: #da vedere: batch size a 1 iterando su ogni elemento del dataset; vedere se va bene montare piu frasi in un solo elemento \n",
        "        print(batch)\n",
        "        batch_t = []\n",
        "        optim.zero_grad()\n",
        "        for b in batch[\"Text\"]:\n",
        "          batch_t = tokenizer(b, return_tensors=\"pt\", truncation=True)\n",
        "          print(type(batch_t))\n",
        "        batch_t.to(device)\n",
        "        outputs = model(**batch_t, labels=batch_t[\"input_ids\"])\n",
        "        loss = outputs[0]\n",
        "        print(loss)\n",
        "        loss.backward()\n",
        "        optim.step()  \n",
        "\n",
        "def format_dataset_v3(annotations,category,tokenizer):\n",
        "  dataset = []\n",
        "  for ann in annotations:\n",
        "    text = category + \" \" + ann['caption']\n",
        "    tokenizer_kwargs = {}\n",
        "    dataset.append(text)\n",
        "  return dataset\n",
        "\n",
        "def finetune_v3(model, tokenizer, device, annotations, category = 'outdoor' ):#,dataset_path = \"/content/drive/MyDrive/Machine Learning/Progetto/prova_1_val.txt\"):\n",
        "  print(\"start finetune\")\n",
        "  train_dataset = []\n",
        "  model.to(device)\n",
        "  len_annotation = len(annotations)\n",
        "  model.train()\n",
        "  #Se non esiste il control code lo aggiungo\n",
        "  encoded_category = tokenizer.encode(category, add_special_tokens=False)\n",
        "  if not any(encoded_category[0] == x for x in tokenizer.control_codes.values()):\n",
        "    tokenizer.control_codes[category] = encoded_category[0]\n",
        "\n",
        "  #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "  dataset = format_dataset_v3(annotations = annotations, category = category, tokenizer = tokenizer)\n",
        "  text_labels_df = pd.DataFrame({'Text': dataset})\n",
        "  TD = TextSet_v3(text_labels_df['Text'])                        \n",
        "  train_loader = DataLoader(TD, batch_size=1, shuffle=True)\n",
        "  optim = AdamW(model.parameters(), lr=1e-4)\n",
        "  \n",
        "\n",
        "  for epoch in range(3):\n",
        "    n_steps = 0\n",
        "    avg = 0\n",
        "    print(\"EPOCH\")\n",
        "    for batch in train_loader: #da vedere: batch size a 1 iterando su ogni elemento del dataset; vedere se va bene montare piu frasi in un solo elemento \n",
        "      optim.zero_grad()\n",
        "      batch_t = tokenizer(batch, return_tensors=\"pt\", truncation=True)\n",
        "      batch_t.to(device)\n",
        "    \n",
        "      outputs = model(**batch_t, labels=batch_t['input_ids'])\n",
        "      loss = outputs[0]\n",
        "      del batch_t\n",
        "      del outputs\n",
        "      torch.cuda.empty_cache()\n",
        "      n_steps = n_steps + 1\n",
        "      avg = avg + loss\n",
        "      if n_steps >= 300:\n",
        "        avg = avg/n_steps\n",
        "        print(avg)\n",
        "        n_steps = 0\n",
        "        avg = 0\n",
        "      loss.backward(retain_graph=False)\n",
        "      optim.step() \n",
        "    avg = avg/n_steps\n",
        "    print(avg) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTWm2FaMvePW"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdpREx4rwKuq"
      },
      "source": [
        "'''\n",
        "def generate(model,tokenizer,device,first_word = \"outdoor \"):\n",
        "  model.eval()\n",
        "  word = \"\"\n",
        "  phrase = \"\"\n",
        "  for k in range(10):\n",
        "    phrase = phrase + word + \" \"\n",
        "    print(\"phrase: \" + phrase)\n",
        "    inputs = tokenizer.encode_plus(first_word + word, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device)\n",
        "    outputs = model(**inputs,labels=inputs[\"input_ids\"])\n",
        "    word =  decode_word(output = outputs)\n",
        "    print(\"word:\" + word[0])\n",
        "  return outputs\n",
        "\n",
        "def decode_word(output):\n",
        "  x = \"\"\n",
        "  loss = output.loss\n",
        "  logits = output.logits\n",
        "  print(logits)\n",
        "  softmax = F.softmax(logits, dim = -1)\n",
        "  index = torch.argmax(softmax,dim = -1)\n",
        "  x = tokenizer.decode(index[0])\n",
        "  return x\n",
        "'''\n",
        "\n",
        "def prepare_ctrl_input(tokenizer, prompt_text):\n",
        "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
        "  if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):\n",
        "      print(tokenizer.control_codes.values())\n",
        "  return prompt_text\n",
        "\n",
        "def generate_v2(model,tokenizer,device,control_code = \"outdoor\"):\n",
        "  #controllo che il control code esista\n",
        "  model.to(device)\n",
        "  preprocessed_prompt_text = prepare_ctrl_input(tokenizer, control_code)\n",
        "  tokenizer_kwargs = {}\n",
        "  encoded_prompt = tokenizer.encode(\n",
        "      preprocessed_prompt_text, add_special_tokens=False, return_tensors=\"pt\", **tokenizer_kwargs\n",
        "  )\n",
        "\n",
        "  if encoded_prompt.size()[-1] == 0:\n",
        "    input_ids = None\n",
        "  else:\n",
        "    input_ids = encoded_prompt\n",
        "  output_sequences = model.generate(\n",
        "        input_ids=input_ids.to(device),\n",
        "        max_length= 50 + len(encoded_prompt[0]),\n",
        "        temperature=0.4,\n",
        "        top_k=1,\n",
        "        top_p=0.8,\n",
        "        repetition_penalty=5.0,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "  print(output_sequences)\n",
        "\n",
        "  if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "  generated_sequences = []\n",
        "\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "      print(f\"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===\")\n",
        "      generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "      # Decode text\n",
        "      text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "      # Remove all text after the stop token\n",
        "      text = text[: None]\n",
        "\n",
        "      # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
        "      total_sequence = (\n",
        "          control_code + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "      )\n",
        "\n",
        "      generated_sequences.append(total_sequence)\n",
        "      print(total_sequence)\n",
        "  return total_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2MbSdIDKgwy"
      },
      "source": [
        "### No texygen evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SvAsNS6KnDi"
      },
      "source": [
        "# BLEU or not BLEU ------>  https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213\n",
        "#UTILE PER COMPRENDERE BLEU N-GRAM https://stackoverflow.com/questions/46444656/bleu-scores-could-i-use-nltk-translate-bleu-score-sentence-bleu-for-calculating\n",
        "\n",
        "def tokenize_annotation(anns):\n",
        "  text = []\n",
        "  pos_text = []\n",
        "  for ann in anns:\n",
        "    #text.append(ann['caption'].split())\n",
        "    sentence = nltk.word_tokenize(ann['caption'])\n",
        "    text.append(sentence)\n",
        "    pos = nltk.pos_tag(sentence)\n",
        "    pos_text.append([el[1] for el in pos])\n",
        "  return text,pos_text\n",
        "\n",
        "def split_sentence(corpus = \"\"):\n",
        "  text = []\n",
        "  pos_text = []\n",
        "  corpus = corpus.replace(\".\",\". \")\n",
        "  d = \".\"\n",
        "  sentences = corpus.split(d)\n",
        "  for sentence in sentences:\n",
        "    #s = sentence.split()\n",
        "    sentence += \".\"\n",
        "    s = nltk.word_tokenize(sentence)\n",
        "    if len(s) > 1:\n",
        "      text.append(s)\n",
        "      pos = nltk.pos_tag(s) #estraggo solo la pos\n",
        "      pos_text.append([el[1] for el in pos])\n",
        "    #return text,corpus.replace(\".\",\" \").split()\n",
        "  return text,nltk.word_tokenize(corpus),pos_text\n",
        "\n",
        "def calculate_bleu(original_text = \"\", generated_text = \"\", sentence_generated_text = \"\",mode = [True,True,True]):\n",
        "  list_ref = []\n",
        "  if mode[0]:\n",
        "    #BLEU score calcolato tra il testo generato messo in una sola lista e il testo di riferimento. Il potenziale problema è che l'analisi 3-gram e 4-gram considera le frasi adiacenti come una cosa unica, quindi cerca di connettere la fine della frase 1 con l'inizio della frase 2, causando un abbassamento dello score.\n",
        "    score = corpus_bleu([original_text], [generated_text],smoothing_function=SmoothingFunction().method4,emulate_multibleu=True,weights = (0.25,0.25,0.25,0.25))\n",
        "    print(\"Overall BLEU Score = \" + str(score)) \n",
        "  total_score = 0\n",
        "  #BLEU score calcolato singolarmente tra una frase del testo generato e il testo di riferimento. Il problema è che tratto le frasi come parti singole, quindi l'analisi 3-gram e 4-gram non viene eseguita tra frasi diverse. L'analisi BLEU non è cumulativa\n",
        "  for sentence in sentence_generated_text:\n",
        "    if mode[1]:\n",
        "      #print(sentence)\n",
        "      score = corpus_bleu([original_text], [sentence],smoothing_function=SmoothingFunction().method4,emulate_multibleu=True,weights = (0.25,0.25,0.25,0.25))\n",
        "      total_score += score\n",
        "    list_ref.append(original_text)\n",
        "  if mode[1]:\n",
        "    print(\"Average on sentences BLEU Score = \" + str(total_score/len(list_ref)))\n",
        "  if mode[2]:\n",
        "    #BLEU Score calcolato cumulativamente tra una singola frase generata e il testo di riferimento. Questa è un'analisi cumulativa che differisce dalla media dei BLEU score calcolati sulle singole frasi. Penso sia questo il modo corretto di calcolarlo\n",
        "    score = corpus_bleu(list_ref, sentence_generated_text,smoothing_function=SmoothingFunction().method4,emulate_multibleu=True,weights = (0.25,0.25,0.25,0.25))\n",
        "    print(\"Cumulative on sentences BLEU Score = \" + str(score))\n",
        "\n",
        "def BLEU_eval(category = ['outdoor'], corpus = \"\", ann = []): \n",
        "  original_text,pos_original = tokenize_annotation(ann)\n",
        "  sentence_generated_text,generated_text,pos_sentence_generated = split_sentence(corpus = corpus)\n",
        "  pos_generated = [el[1] for el in nltk.pos_tag(generated_text)]\n",
        "\n",
        "  print(\"===== BLEU SCORE =====\")\n",
        "  calculate_bleu(original_text = original_text, generated_text = generated_text, sentence_generated_text = sentence_generated_text,mode = [True,True,True])\n",
        "  print(\"===== SELF BLEU SCORE =====\")\n",
        "  calculate_bleu(original_text = generated_text, generated_text = generated_text, sentence_generated_text = sentence_generated_text, mode = [True,True,True])\n",
        "  print(\"===== POS BLEU SCORE =====\")\n",
        "  calculate_bleu(original_text = pos_original, generated_text = pos_generated, sentence_generated_text = pos_sentence_generated,mode = [True,True,True])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5JkIOUMRMng"
      },
      "source": [
        "sia in bleu che in self bleu get_scoore() e get_bleu() sembrano le funzioni principali da richiamare. Le altre sono chiamate tutte da esse. Capire la differenza tra le 2\n",
        "\n",
        "BLEU -> applicare il sentence_bleu (o corpus_bleu) tra il testo generato e quello di partenza, bigger is better\n",
        "              link texygen BLEU -> https://github.com/geek-ai/Texygen/blob/master/utils/metrics/Bleu.py\n",
        "SELF BLEU -> applicare il sentence_bleu (o corpus_bleu) tra il testo generato e se stesso, lower is better (?) da controllare\n",
        "              link texygen self bleu -> https://github.com/geek-ai/Texygen/blob/master/utils/metrics/SelfBleu.py\n",
        "              \n",
        "POS BLEU -> \n",
        "s = t1, t2, ...., tn\n",
        "t1 -> significato grammaticale == Noun\n",
        "t2 == Verb \n",
        "....\n",
        "POS BLEU : applicare il bleu non sui token ma sulle POS (noun, verb, ...) --> ANALISI LOGICA\n",
        "\n",
        "how to extract parts of speech (POS) --> https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN9R8LuTwSP9"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqh58YhEwVmj"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "import torch\n",
        "\n",
        "choosen_cat = \"\"\n",
        "pretrained = False\n",
        "save_model = True\n",
        "if os.path.isfile('/content/drive/MyDrive/Machine Learning/Progetto/Models/vanilla10/pytorch_model.bin'):\n",
        "  pretrained = True\n",
        "if True:#not pretrained:\n",
        "  coco,coco_ann = init_COCO()\n",
        "  cats = get_categories(coco = coco)\n",
        "else: #se è pretrainato uso la lista per non obbligare l'utente ad avere le annotazioni scaricate\n",
        "  supercats = ['outdoor', 'person', 'kitchen', 'furniture', 'food', 'animal', 'appliance', 'accessory', 'vehicle', 'indoor', 'sports', 'electronic']\n",
        "model,tokenizer,device = init(pretrained)\n",
        "for cat in ['outdoor']:\n",
        "  ann = get_annotation(coco = coco, coco_ann = coco_ann, catNms = [cat]) \n",
        "  if True:#not pretrained:\n",
        "    finetune_v3(model = model,tokenizer = tokenizer,device = device, annotations = ann, category = cat)\n",
        "if save_model:\n",
        "  model.save_pretrained(\"/content/drive/MyDrive/Machine Learning/Progetto/Models/outdoor/person\")\n",
        "while choosen_cat != \"exit\":\n",
        "  choosen_cat = choose_cat(cats)\n",
        "  if choosen_cat != \"exit\":\n",
        "    output = generate_v2(model = model, tokenizer = tokenizer, device = device, control_code = choosen_cat)\n",
        "    ann = get_annotation(coco = coco, coco_ann = coco_ann, catNms = choosen_cat)\n",
        "    BLEU_eval(category = choosen_cat, corpus = output, ann = ann)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypitAfHyY9aT"
      },
      "source": [
        "While True:\n",
        "  a = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5lxwLgAuD1A"
      },
      "source": [
        "# Texygen evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P_mgDGfqm-x"
      },
      "source": [
        "**Where the execution being stopped raising errors concerned about tensorflow,  replace `import tensorflow as tf` with `import tensorflow.compat.v1 as tf` and adding `tf.disable_v2_behavior()` before the code**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gExeLaseQ9aZ"
      },
      "source": [
        "def download_texygen():\n",
        "  !pip install colorama\n",
        "\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  project_path = '/content/drive/MyDrive/Machine Learning/Progetto'\n",
        "  os.chdir(project_path)\n",
        "  if not os.path.isdir('/content/drive/MyDrive/Machine Learning/Progetto/Texygen'):\n",
        "    !git clone https://github.com/geek-ai/Texygen.git\n",
        "\n",
        "def run_texygen(gan = \"mle\", training = \"oracle\", data = \"./data/image_coco.txt\"):\n",
        "  texygen_path = '/content/drive/MyDrive/Machine Learning/Progetto/Texygen'\n",
        "  os.chdir(texygen_path)\n",
        "  !python3 main.py -g $gan -t $training -d $data\n",
        "\n",
        "def plot_results_texygen(gan = \"mle\", training = \"oracle\", data = \"./data/image_coco.txt\"): \n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  base_path = \"/content/drive/MyDrive/Machine Learning/Progetto/Texygen/experiment-log-\" + gan\n",
        "  suffix_path = \"-\" + training + \".csv\" if training != \"oracle\" else \".csv\"\n",
        "  result_file_path = base_path + suffix_path\n",
        "\n",
        "  print(\"READING FILE: \" + result_file_path)\n",
        "  print(\"\")\n",
        "  results = pd.read_csv(result_file_path)\n",
        "  results.dropna(axis = \"columns\", inplace = True)\n",
        "  print(results)\n",
        "\n",
        "  epochs = results.iloc[:, 0]\n",
        "  metrics = {}\n",
        "  for index in range(1, len(results.columns)):  \n",
        "    metrics[\"metric{0}\".format(index)] = results.iloc[:, index]\n",
        "\n",
        "  with plt.style.context(\"Solarize_Light2\"):\n",
        "    for key, value in metrics.items():\n",
        "      plt.plot(epochs, value, \"-b\", marker=\"o\", label = value.name)\n",
        "      plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3GExzviWu8e"
      },
      "source": [
        "download_texygen()\n",
        "run_texygen(\"leakgan\", \"real\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwR0UEGcG2td"
      },
      "source": [
        "plot_results_texygen(\"seqgan\", \"real\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZgEMzUxtAwR"
      },
      "source": [
        "#API e salesforce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_kIPTSmvU-Z"
      },
      "source": [
        "### Prepare model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSMBHIfUxOeb"
      },
      "source": [
        "from shutil import copyfile\n",
        "import gc\n",
        "\n",
        "# --- INITIALIZING SOME CONSTANT ----\n",
        "\n",
        "lowerMemory = False\n",
        "mixed = False\n",
        "project_path = \"/content/drive/MyDrive/Machine Learning/Progetto/\"\n",
        "ctrl_path = project_path + \"ctrl/\"\n",
        "training_path = ctrl_path + \"training_utils/\"\n",
        "pretrained_model_dir = \"model/\"\n",
        "model_name = \"model.ckpt-684000.data-00000-of-00001\"\n",
        "model_dir = \"seqlen256_36layers_v0.ckpt/\"  \n",
        "\n",
        "def Patch_Keras(old_path = ctrl_path):\n",
        "  os.chdir(ctrl_path)\n",
        "  !patch -b /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/keras.py estimator.patch\n",
        "  os.chdir(old_path)\n",
        "\n",
        "def init_no_API():\n",
        "  os.chdir(project_path) #%cd project_path\n",
        "  !nvidia-smi\n",
        "  !pip2 install tensorflow-gpu==1.14 #installo le dipendenze\n",
        "  prova = training_path + \"prova1.txt\"\n",
        "  !rm -rf ctrl\n",
        "  !rm -rf prova\n",
        "  !git clone https://github.com/salesforce/ctrl #clono CTRL\n",
        "  os.chdir(ctrl_path) #%cd ctrl_path\n",
        "  if lowerMemory == True:\n",
        "    Path_Keras(ctrl_path)\n",
        "    !git checkout lower_memory\n",
        "  else:\n",
        "    !pip install torch\n",
        "    !git checkout master\n",
        "  !pip2 install fastBPE\n",
        "\n",
        "  if mixed == False:\n",
        "    if os.path.isfile(project_path + model_dir + model_name) == False:\n",
        "      os.chdir(project_path)#%cd project_path\n",
        "      !pip2 install gsutil #gsutil is a Python application that lets you access Cloud Storage from the command line. You can use gsutil to do a wide range of bucket and object management tasks\n",
        "      !gsutil -m cp -r gs://sf-ctrl/seqlen256_36layers_v0.ckpt/ .\n",
        "      #!gsutil -m cp -r gs://sf-ctrl/seqlen256_v1.ckpt . #48 layer\n",
        "      #()\n",
        "  else:\n",
        "    print(\"MIXED @@@@@@@@@@@@@@@@\")\n",
        "    #init_mixed()\n",
        "\n",
        "def generate_no_API():\n",
        "  gc.collect()\n",
        "  os.chdir(ctrl_path)#%cd ctrl_path\n",
        "  if lowerMemory == True:\n",
        "    !python2 generation.py --model ../seqlen256_v1.ckpt/model.ckpt-413000.data-00000-of-00001\n",
        "  else:\n",
        "    !python2 pytorch_generation.py --temperature 0.5 --model ../seqlen256_v1.ckpt/model.ckpt-684000.data-00000-of-00001\n",
        "\n",
        "def init_API():\n",
        "  !pip install transformers\n",
        "  from transformers import TFCTRLLMHeadModel, CTRLConfig, CTRLTokenizer\n",
        "  import torch\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  tokenizer = CTRLTokenizer.from_pretrained('ctrl')\n",
        "  model = TFCTRLLMHeadModel.from_pretrained('ctrl', config = CTRLConfig(n_layer = 10))\n",
        "  \n",
        "  return model,tokenizer,device\n",
        "\n",
        "def generate_API(model,tokenizer,device):\n",
        "  inputs = tokenizer(\"Politics Title:\", return_tensors=\"tf\")\n",
        "  outputs = model(inputs)\n",
        "  return outputs\n",
        "\n",
        "def finetune_CTRL_API():\n",
        "  #farò il finetune e salverò il modello su drive, cancellando la cartella del API = false\n",
        "  #nel metodo API = False dovrò cancellare questo modello\n",
        "  i = 50\n",
        "\n",
        "def Create_tf_records(file_list = [(\"outdoor\",\"prova_1.txt\")]): \n",
        "  #avrò una lista con [(nome_label,path_file),...]. dovrò leggere path_file da google drive e produrrò una lista con il path al file contenente il tf record\n",
        "  Patch_Keras(training_path)  \n",
        "  #os.system(\"wget -O moby_dick.txt https://www.gutenberg.org/files/2701/2701-0.txt\") #DEBUG, per le prove finchè non avrò la lista\n",
        "  #mi salva i record in moby_dick.txt.tfrecords\n",
        "  for (control_f,path_f) in file_list:\n",
        "    copyfile(project_path + path_f, training_path + path_f)\n",
        "    path_file = training_path + path_f + \".tfrecords\"\n",
        "    if os.path.isfile(path_file) == False:\n",
        "      #se non esiste il file tfrecords devo crearlo\n",
        "      command_create_tf = \"python2 make_tf_records.py --text_file {0} --control_code {1} --sequence_len 256\".format(path_f,control_f)\n",
        "      subprocess.check_output(command_create_tf,shell=True)\n",
        "      print(\"Creato file con control code {0}\".format(control_f))\n",
        "\n",
        "def finetune_CTRL_no_API(file_list = []):\n",
        "  Create_tf_records()\n",
        "  #train_cmd = \"python2 training.py --model_dir {} --iterations 1\".format(project_path.replace(\" \",\"\\\\ \") + model_dir)\n",
        "  print(\"START_TRAINING\")\n",
        "  gc.collect()\n",
        "  #subprocess.check_output(train_cmd,shell = True)\n",
        "  !python2 training.py --model_dir /content/drive/MyDrive/Machine\\ Learning/Progetto/seqlen256_36layers_v0.ckpt/ --iterations 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ge12-0qjrOx"
      },
      "source": [
        "### Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2fszJLmzo8D"
      },
      "source": [
        "use_api = True\n",
        "finetune_CTRL = True\n",
        "generate_CTRL = False\n",
        "if use_api == False:\n",
        "  init_no_API()\n",
        "  if finetune_CTRL == True:\n",
        "    finetune_CTRL_no_API()\n",
        "  if generate_CTRL == True:\n",
        "    generate_no_API()\n",
        "else:\n",
        "  model,tokenizer,device = init_API()\n",
        "  output = generate_API(model = model,tokenizer = tokenizer, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}